{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Calabria_Day1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heckelei/PromotionskollegModule6800_2020/blob/master/6800_Day1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71QCGinKTAAf",
        "colab_type": "text"
      },
      "source": [
        "# Day 1 : Code used during lecture and lab assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkq_ugJnc0qD",
        "colab_type": "text"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "- The notebook combines 'code used during lecture' with the 'Day 1 lab' assignment (see further down)\n",
        "- The lab assignment can be done largely by copying/paste/modification of the code used during the lecture\n",
        "- Please add answers/discussion/comments to the notebook as comments or text box. Do not create another file in addition.\n",
        "- When you are done with your assignment, save the notebook in drive and add your last name to the name of the file\n",
        "- Upload your final notebook to https://uni-bonn.sciebo.de/s/mTpqLLBN9Wu71Ku latest by September 30th. The password for access was sent to you by email"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBjm-4D-Tg-L",
        "colab_type": "text"
      },
      "source": [
        "# Code used during lecture\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn9LO_jCSar8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bd6739fe-3689-4b66-e278-583ca6721d7c"
      },
      "source": [
        "# Import libaries that will be used in the notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0aKGu2ArXhh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f7627624-06b0-45d9-c235-286b3473944a"
      },
      "source": [
        "# Download data\n",
        "!wget http://www.ilr.uni-bonn.de/agpo/courses/ml/brazil_all_data_v2.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-24 11:04:52--  http://www.ilr.uni-bonn.de/agpo/courses/ml/brazil_all_data_v2.gz\n",
            "Resolving www.ilr.uni-bonn.de (www.ilr.uni-bonn.de)... 131.220.69.2\n",
            "Connecting to www.ilr.uni-bonn.de (www.ilr.uni-bonn.de)|131.220.69.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 283350352 (270M) [application/x-gzip]\n",
            "Saving to: ‘brazil_all_data_v2.gz’\n",
            "\n",
            "brazil_all_data_v2. 100%[===================>] 270.22M  6.48MB/s    in 47s     \n",
            "\n",
            "2020-08-24 11:05:39 (5.78 MB/s) - ‘brazil_all_data_v2.gz’ saved [283350352/283350352]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mystC39Lldkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data with pandas into a dataframe \n",
        "df = pd.read_parquet('brazil_all_data_v2.gz')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snUuhtBAmtnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "a07c02e5-0523-4e71-da22-4de5f5d86b75"
      },
      "source": [
        "# Have a look at the data\n",
        "print('Number of rows:', df.shape[0])\n",
        "df.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows: 249940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>row</th>\n",
              "      <th>col</th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>bean</th>\n",
              "      <th>carrot</th>\n",
              "      <th>cassava</th>\n",
              "      <th>chickpea</th>\n",
              "      <th>citrus</th>\n",
              "      <th>coffee</th>\n",
              "      <th>groundnut</th>\n",
              "      <th>maize</th>\n",
              "      <th>soy</th>\n",
              "      <th>sugarcane</th>\n",
              "      <th>tomato</th>\n",
              "      <th>wheat</th>\n",
              "      <th>perc_treecover</th>\n",
              "      <th>perm_water</th>\n",
              "      <th>travel_min</th>\n",
              "      <th>defor_2001</th>\n",
              "      <th>defor_2002</th>\n",
              "      <th>defor_2003</th>\n",
              "      <th>defor_2004</th>\n",
              "      <th>defor_2005</th>\n",
              "      <th>defor_2006</th>\n",
              "      <th>defor_2007</th>\n",
              "      <th>defor_2008</th>\n",
              "      <th>defor_2009</th>\n",
              "      <th>defor_2010</th>\n",
              "      <th>defor_2011</th>\n",
              "      <th>defor_2012</th>\n",
              "      <th>defor_2013</th>\n",
              "      <th>defor_2014</th>\n",
              "      <th>defor_2015</th>\n",
              "      <th>defor_2016</th>\n",
              "      <th>defor_2017</th>\n",
              "      <th>defor_2018</th>\n",
              "      <th>wdpa_1990</th>\n",
              "      <th>wdpa_1991</th>\n",
              "      <th>...</th>\n",
              "      <th>tot_defor_2016_lag_1st_order</th>\n",
              "      <th>tot_defor_2017_lag_1st_order</th>\n",
              "      <th>tot_defor_2018_lag_1st_order</th>\n",
              "      <th>tot_defor_2001_lag_2nd_order</th>\n",
              "      <th>tot_defor_2002_lag_2nd_order</th>\n",
              "      <th>tot_defor_2003_lag_2nd_order</th>\n",
              "      <th>tot_defor_2004_lag_2nd_order</th>\n",
              "      <th>tot_defor_2005_lag_2nd_order</th>\n",
              "      <th>tot_defor_2006_lag_2nd_order</th>\n",
              "      <th>tot_defor_2007_lag_2nd_order</th>\n",
              "      <th>tot_defor_2008_lag_2nd_order</th>\n",
              "      <th>tot_defor_2009_lag_2nd_order</th>\n",
              "      <th>tot_defor_2010_lag_2nd_order</th>\n",
              "      <th>tot_defor_2011_lag_2nd_order</th>\n",
              "      <th>tot_defor_2012_lag_2nd_order</th>\n",
              "      <th>tot_defor_2013_lag_2nd_order</th>\n",
              "      <th>tot_defor_2014_lag_2nd_order</th>\n",
              "      <th>tot_defor_2015_lag_2nd_order</th>\n",
              "      <th>tot_defor_2016_lag_2nd_order</th>\n",
              "      <th>tot_defor_2017_lag_2nd_order</th>\n",
              "      <th>tot_defor_2018_lag_2nd_order</th>\n",
              "      <th>tot_defor_2001_lag_3rd_order</th>\n",
              "      <th>tot_defor_2002_lag_3rd_order</th>\n",
              "      <th>tot_defor_2003_lag_3rd_order</th>\n",
              "      <th>tot_defor_2004_lag_3rd_order</th>\n",
              "      <th>tot_defor_2005_lag_3rd_order</th>\n",
              "      <th>tot_defor_2006_lag_3rd_order</th>\n",
              "      <th>tot_defor_2007_lag_3rd_order</th>\n",
              "      <th>tot_defor_2008_lag_3rd_order</th>\n",
              "      <th>tot_defor_2009_lag_3rd_order</th>\n",
              "      <th>tot_defor_2010_lag_3rd_order</th>\n",
              "      <th>tot_defor_2011_lag_3rd_order</th>\n",
              "      <th>tot_defor_2012_lag_3rd_order</th>\n",
              "      <th>tot_defor_2013_lag_3rd_order</th>\n",
              "      <th>tot_defor_2014_lag_3rd_order</th>\n",
              "      <th>tot_defor_2015_lag_3rd_order</th>\n",
              "      <th>tot_defor_2016_lag_3rd_order</th>\n",
              "      <th>tot_defor_2017_lag_3rd_order</th>\n",
              "      <th>tot_defor_2018_lag_3rd_order</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-59.989876</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>237.00000</td>\n",
              "      <td>115.0</td>\n",
              "      <td>461.00000</td>\n",
              "      <td>209.00000</td>\n",
              "      <td>1295.0000</td>\n",
              "      <td>357.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.761093</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2612.6440</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.866667</td>\n",
              "      <td>6.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.625000</td>\n",
              "      <td>26.499998</td>\n",
              "      <td>17.500000</td>\n",
              "      <td>2.625000</td>\n",
              "      <td>2.125000</td>\n",
              "      <td>37.375000</td>\n",
              "      <td>1.125000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>11.625000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>4.125000</td>\n",
              "      <td>4.875000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>9.875000</td>\n",
              "      <td>1.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>14.333333</td>\n",
              "      <td>9.800000</td>\n",
              "      <td>1.533333</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.800000</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>6.866667</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>4.466667</td>\n",
              "      <td>9.866667</td>\n",
              "      <td>6.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-59.969875</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>237.00000</td>\n",
              "      <td>115.0</td>\n",
              "      <td>461.00000</td>\n",
              "      <td>209.00000</td>\n",
              "      <td>1295.0000</td>\n",
              "      <td>357.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.777657</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2680.3191</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013125</td>\n",
              "      <td>0.008437</td>\n",
              "      <td>0.011875</td>\n",
              "      <td>0.003125</td>\n",
              "      <td>0.000781</td>\n",
              "      <td>0.046719</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.002812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002188</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>0.006094</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.006562</td>\n",
              "      <td>0.001406</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.473684</td>\n",
              "      <td>6.210527</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>19.999998</td>\n",
              "      <td>5.818181</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>1.363636</td>\n",
              "      <td>1.545454</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>1.818182</td>\n",
              "      <td>7.909091</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1.636364</td>\n",
              "      <td>1.272727</td>\n",
              "      <td>12.909090</td>\n",
              "      <td>10.181818</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>12.052631</td>\n",
              "      <td>3.842105</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.052631</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.105263</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.473684</td>\n",
              "      <td>9.473684</td>\n",
              "      <td>6.210527</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-59.949875</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>237.00000</td>\n",
              "      <td>115.0</td>\n",
              "      <td>461.00000</td>\n",
              "      <td>209.00000</td>\n",
              "      <td>1295.0000</td>\n",
              "      <td>357.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.766403</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2796.3284</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.024531</td>\n",
              "      <td>0.009375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001719</td>\n",
              "      <td>0.005313</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.695652</td>\n",
              "      <td>11.217392</td>\n",
              "      <td>5.173913</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.214286</td>\n",
              "      <td>8.785713</td>\n",
              "      <td>5.857143</td>\n",
              "      <td>1.642857</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>22.571428</td>\n",
              "      <td>1.785714</td>\n",
              "      <td>2.214286</td>\n",
              "      <td>2.571429</td>\n",
              "      <td>7.142858</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.357143</td>\n",
              "      <td>3.785714</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>8.571429</td>\n",
              "      <td>3.142857</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>1.826087</td>\n",
              "      <td>6.869565</td>\n",
              "      <td>7.086957</td>\n",
              "      <td>8.260869</td>\n",
              "      <td>1.782609</td>\n",
              "      <td>4.347826</td>\n",
              "      <td>18.043478</td>\n",
              "      <td>1.956522</td>\n",
              "      <td>3.652174</td>\n",
              "      <td>1.652174</td>\n",
              "      <td>5.913043</td>\n",
              "      <td>4.086957</td>\n",
              "      <td>4.521739</td>\n",
              "      <td>4.956522</td>\n",
              "      <td>8.695652</td>\n",
              "      <td>11.217392</td>\n",
              "      <td>5.173913</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-59.929874</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>237.00000</td>\n",
              "      <td>115.0</td>\n",
              "      <td>461.00000</td>\n",
              "      <td>209.00000</td>\n",
              "      <td>1295.0000</td>\n",
              "      <td>357.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.814842</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2920.0164</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002188</td>\n",
              "      <td>0.017812</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.888889</td>\n",
              "      <td>19.629629</td>\n",
              "      <td>6.518518</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>2.928571</td>\n",
              "      <td>11.285714</td>\n",
              "      <td>18.214285</td>\n",
              "      <td>17.214285</td>\n",
              "      <td>2.928571</td>\n",
              "      <td>6.142857</td>\n",
              "      <td>28.428572</td>\n",
              "      <td>3.142857</td>\n",
              "      <td>5.357143</td>\n",
              "      <td>2.785714</td>\n",
              "      <td>8.428572</td>\n",
              "      <td>5.857142</td>\n",
              "      <td>7.214285</td>\n",
              "      <td>6.142857</td>\n",
              "      <td>6.785714</td>\n",
              "      <td>15.785715</td>\n",
              "      <td>8.214286</td>\n",
              "      <td>1.222222</td>\n",
              "      <td>2.777778</td>\n",
              "      <td>5.925926</td>\n",
              "      <td>12.185184</td>\n",
              "      <td>10.740741</td>\n",
              "      <td>2.074074</td>\n",
              "      <td>4.370370</td>\n",
              "      <td>15.555556</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>3.814815</td>\n",
              "      <td>2.666667</td>\n",
              "      <td>5.407407</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.925926</td>\n",
              "      <td>3.703704</td>\n",
              "      <td>5.888889</td>\n",
              "      <td>19.629629</td>\n",
              "      <td>6.518518</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-59.909874</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>218.33334</td>\n",
              "      <td>435.83334</td>\n",
              "      <td>216.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>523.5</td>\n",
              "      <td>317.83331</td>\n",
              "      <td>117.5</td>\n",
              "      <td>522.66663</td>\n",
              "      <td>233.16666</td>\n",
              "      <td>1300.8334</td>\n",
              "      <td>482.83331</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.655937</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2977.4216</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005469</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.888889</td>\n",
              "      <td>18.888889</td>\n",
              "      <td>5.222222</td>\n",
              "      <td>2.357143</td>\n",
              "      <td>5.285714</td>\n",
              "      <td>5.285714</td>\n",
              "      <td>14.785714</td>\n",
              "      <td>14.357142</td>\n",
              "      <td>2.571429</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>7.571428</td>\n",
              "      <td>2.642857</td>\n",
              "      <td>5.285714</td>\n",
              "      <td>3.857143</td>\n",
              "      <td>3.642857</td>\n",
              "      <td>6.142857</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.357143</td>\n",
              "      <td>15.285714</td>\n",
              "      <td>29.714287</td>\n",
              "      <td>9.142857</td>\n",
              "      <td>10.185184</td>\n",
              "      <td>3.111111</td>\n",
              "      <td>5.925926</td>\n",
              "      <td>9.777778</td>\n",
              "      <td>10.740741</td>\n",
              "      <td>2.148148</td>\n",
              "      <td>4.111111</td>\n",
              "      <td>15.000001</td>\n",
              "      <td>1.851852</td>\n",
              "      <td>8.296296</td>\n",
              "      <td>2.629630</td>\n",
              "      <td>5.222222</td>\n",
              "      <td>7.592592</td>\n",
              "      <td>5.370370</td>\n",
              "      <td>4.481482</td>\n",
              "      <td>8.888889</td>\n",
              "      <td>18.888889</td>\n",
              "      <td>5.222222</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 426 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  row  col  ...  tot_defor_2017_lag_3rd_order  tot_defor_2018_lag_3rd_order  s\n",
              "0   0    0    0  ...                      6.600000                      0.800000  1\n",
              "1   1    0    1  ...                      6.210527                      2.000000  1\n",
              "2   2    0    2  ...                     11.217392                      5.173913  1\n",
              "3   3    0    3  ...                     19.629629                      6.518518  1\n",
              "4   4    0    4  ...                     18.888889                      5.222222  1\n",
              "\n",
              "[5 rows x 426 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2ZGZoInoHAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define target (dependent) variable (% forest cover for 2018)\n",
        "strY = 'perc_treecover'\n",
        "\n",
        "\n",
        "# Define a list of features names (explantory variables)\n",
        "lstX = [\n",
        "  'wdpa_2017',\n",
        "  'population_2015',\n",
        "  'chirps_2017',\n",
        "  'maize',\n",
        "  'soy',\n",
        "  'sugarcane',\n",
        "  'perm_water',\n",
        "  'travel_min',\n",
        "  'cropland',\n",
        "  'mean_elev',\n",
        "  'sd_elev',\n",
        "  'near_road',\n",
        " ]\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRd3KIDXp152",
        "colab_type": "text"
      },
      "source": [
        "Run OLS on forest cover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkQjti6z028V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get target variable and features\n",
        "Y_all = df[strY]\n",
        "X_all = df.loc[:,lstX]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSjHfg6PmxDA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1f7072a-68cf-47a0-9004-8a4e5ad1e3e3"
      },
      "source": [
        "# Run OLS using sklearn\n",
        "# We run an regression using sklearn which is one of the most popular \n",
        "# libaries for machine learning \n",
        " \n",
        "# Define model (automatically add a constant and normalize the data) \n",
        "regOls = LinearRegression(fit_intercept=True, normalize=True)\n",
        "# Fit model\n",
        "regOls.fit(X_all, Y_all)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toJ0q4YvoEs9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "08d49a94-46fa-4857-bd56-0c3c44724a81"
      },
      "source": [
        "from scipy.stats import norm\n",
        "# View regression results\n",
        "# Note: Sklearn is not econometric package and does not provide our classical \n",
        "# regression table output. This is not what the machine learning community\n",
        "# looks at. However, we can calculate these things manually.\n",
        "\n",
        "# Get coef\n",
        "coefs = np.hstack((regOls.intercept_,regOls.coef_))\n",
        "\n",
        "N = Y_all.shape[0]\n",
        "K = coefs.shape[0]\n",
        "\n",
        "# Get predicted values y hat\n",
        "Y_hat = regOls.predict(X_all)\n",
        "# Get errors\n",
        "err = Y_all-Y_hat\n",
        "# Get standard error of regression\n",
        "sig2 = (err.transpose() @ err) / (N-K)\n",
        "sig2\n",
        "\n",
        "# Add constant to X_all (sklearn did this automatically)\n",
        "Xc_all = np.insert(np.array(X_all), 0, 1, axis = 1)\n",
        "\n",
        "# Covariance matrix for coef\n",
        "VarBeta = sig2 * np.linalg.inv(Xc_all.T @ Xc_all)\n",
        "# Standard error of coef\n",
        "se = np.sqrt(np.diag(VarBeta))\n",
        "# t-values\n",
        "t =  coefs/se\n",
        "# p-values  \n",
        "p = (1 - norm.cdf(abs(t))) * 2\n",
        "# Prepare df as output\n",
        "resOls = pd.DataFrame(coefs,index=['const']+lstX,columns=['beta'])\n",
        "resOls['SE'] = se\n",
        "resOls['t'] = t\n",
        "resOls['p-value'] = p\n",
        "resOls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beta</th>\n",
              "      <th>SE</th>\n",
              "      <th>t</th>\n",
              "      <th>p-value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>const</th>\n",
              "      <td>-5.831235</td>\n",
              "      <td>1.324112</td>\n",
              "      <td>-4.403882</td>\n",
              "      <td>0.000011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wdpa_2017</th>\n",
              "      <td>11.427969</td>\n",
              "      <td>0.156940</td>\n",
              "      <td>72.817276</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>population_2015</th>\n",
              "      <td>-0.008522</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>-12.595832</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chirps_2017</th>\n",
              "      <td>0.036649</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>116.867636</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maize</th>\n",
              "      <td>0.019095</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>120.445045</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soy</th>\n",
              "      <td>-0.030865</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>-75.758273</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sugarcane</th>\n",
              "      <td>-0.003199</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>-22.739455</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perm_water</th>\n",
              "      <td>-21.908606</td>\n",
              "      <td>1.184469</td>\n",
              "      <td>-18.496556</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel_min</th>\n",
              "      <td>0.019945</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>143.456389</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cropland</th>\n",
              "      <td>-1.174515</td>\n",
              "      <td>0.540421</td>\n",
              "      <td>-2.173335</td>\n",
              "      <td>0.029755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean_elev</th>\n",
              "      <td>-0.023366</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>-51.999590</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sd_elev</th>\n",
              "      <td>0.417715</td>\n",
              "      <td>0.004348</td>\n",
              "      <td>96.080777</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>near_road</th>\n",
              "      <td>0.069442</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>60.442058</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      beta        SE           t   p-value\n",
              "const            -5.831235  1.324112   -4.403882  0.000011\n",
              "wdpa_2017        11.427969  0.156940   72.817276  0.000000\n",
              "population_2015  -0.008522  0.000677  -12.595832  0.000000\n",
              "chirps_2017       0.036649  0.000314  116.867636  0.000000\n",
              "maize             0.019095  0.000159  120.445045  0.000000\n",
              "soy              -0.030865  0.000407  -75.758273  0.000000\n",
              "sugarcane        -0.003199  0.000141  -22.739455  0.000000\n",
              "perm_water      -21.908606  1.184469  -18.496556  0.000000\n",
              "travel_min        0.019945  0.000139  143.456389  0.000000\n",
              "cropland         -1.174515  0.540421   -2.173335  0.029755\n",
              "mean_elev        -0.023366  0.000449  -51.999590  0.000000\n",
              "sd_elev           0.417715  0.004348   96.080777  0.000000\n",
              "near_road         0.069442  0.001149   60.442058  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUQqGRpAYG1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To confirm the results we can use the OLS function in the statsmodel libary.\n",
        "# This is more a statistical libary, not typically used for machine learning, \n",
        "# but providing our typicall regression output\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "olsStats = sm.OLS(Y_all, np.insert(np.array(X_all), 0, 1, axis = 1))\n",
        "# Set the names of the explanatory variables\n",
        "olsStats.data.xnames = ['const']+lstX\n",
        "olsStats_result = olsStats.fit()\n",
        "olsStats_result.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQjoOnOt3_7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we want to see how good our OLS regression performs in sample\n",
        "# i.e. on the training data itself\n",
        "\n",
        "# Make predictions for the same dataset X_all\n",
        "Y_hat = regOls.predict(X_all)\n",
        "\n",
        "# The mean squared error\n",
        "mse_ols_sklearn  = mean_squared_error(Y_all,Y_hat)\n",
        "print('\\nMean squared error: ',mse_ols_sklearn)\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "R2_ols_sklearn = r2_score(Y_all,Y_hat)\n",
        "print('Coefficient of determination: ',R2_ols_sklearn)\n",
        "\n",
        "# plot Y vs Y-hat\n",
        "h = sns.jointplot(Y_hat, Y_all, kind=\"hex\")\n",
        "h.set_axis_labels('Y predicted', 'Y true');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh5EZajJsA8N",
        "colab_type": "text"
      },
      "source": [
        "Now we explore what consequences it has if a model overfits. For this we generate interaction and squared terms and compare model performance in a training and test set approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MgnKOu0M5xH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set a random number seed such that everybody has the same \"random\" split \n",
        "# of the data\n",
        "np.random.seed(111)\n",
        "# Split the data into train and test data using sklearn train_test_split object\n",
        "#   (see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "\n",
        "#   Note: This randomly split the data in 80% train and 20% test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ3is0GFj8k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In order to better illustrate the overfitting effects we \n",
        "# artificially reduce the training data size.  Why do we do this? In general \n",
        "# a larger sample size helps to reduce overfitting problems. With a smaller \n",
        "# sample size polynomials of order 2 of our variables are sufficient to show \n",
        "# the effects. With the full sample size we would need to consider much higher \n",
        "# polynomials to show the same effects. \n",
        "N = 2000\n",
        "X_train = X_train.iloc[:N,:]\n",
        "Y_train = Y_train.iloc[:N]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZpKA77RMr2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use an sklearn function to generate polynomials of order 2 \n",
        "# (square terms and interaction terms)\n",
        "# (see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures.get_feature_names)\n",
        "poly = PolynomialFeatures(2)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "print('Total number of avaliable features',X_train_poly.shape[1])\n",
        "lstFeatures = poly.get_feature_names()\n",
        "# Show feature names\n",
        "# list(lstFeatures)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dKToEAbrNXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To explore overfitting we now run an small simulation...\n",
        "# we squentially increase our model complexity by adding on additional features. \n",
        "# In each case we estimate the model and obtain models stats\n",
        "# for the train and test set.\n",
        "\n",
        "# Prepare a dataframe to hold the results\n",
        "res = pd.DataFrame()\n",
        "\n",
        "# Each iteration add in one  additional variables\n",
        "for numVar in range(0,X_train_poly.shape[1]):\n",
        "\n",
        "  # Define the feature set for the iteration \n",
        "  X_train_subSet = X_train_poly[:,:numVar+1]\n",
        "  X_test_subSet = X_test_poly[:,:numVar+1]\n",
        "  \n",
        "  # Create linear regression object\n",
        "  regOls = LinearRegression(normalize=True)\n",
        "\n",
        "  # Train the model using the training sets\n",
        "  regOls.fit(X_train_subSet, Y_train)\n",
        "\n",
        "  # Get predicted values\n",
        "  Y_hat_train = regOls.predict(X_train_subSet)\n",
        "  Y_hat_test = regOls.predict(X_test_subSet)\n",
        "\n",
        "  # Store model stats \n",
        "  res.loc[f\"numVar_{numVar}\",'Number of X Variables'] = X_train_subSet.shape[1]\n",
        "  res.loc[f\"numVar_{numVar}\",'New Feature'] = lstFeatures[numVar]\n",
        "\n",
        "  # The mean squared error\n",
        "  res.loc[f\"numVar_{numVar}\",'MSE Train'] = mean_squared_error(Y_train,Y_hat_train)\n",
        "  res.loc[f\"numVar_{numVar}\",'MSE Test'] = mean_squared_error(Y_test,Y_hat_test)\n",
        "  # The coefficient of determination: 1 is perfect prediction\n",
        "  res.loc[f\"numVar_{numVar}\",'R2 train'] = r2_score(Y_train,Y_hat_train)\n",
        "  res.loc[f\"numVar_{numVar}\",'R2 test'] = r2_score(Y_test,Y_hat_test)\n",
        "\n",
        "\n",
        "# => The column \"New Feature\" says which feature was added in this iteration \n",
        "#    on top of all the other added before, starting with only a constant\n",
        "# => Have a look how R2 and MSE develops in the train and test set when \n",
        "#    increasing model complexity.\n",
        "res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J13_mhypxJ25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Before we continue lets check if our simulation is correct, by comparing\n",
        "# row \"numVar_12\" from \"res\" with our statsmodel and sklearn that we \n",
        "# have use above\n",
        "print('Results from statsmodel:')\n",
        "print('R2',olsStats_result.rsquared)\n",
        "print('MSE',olsStats_result.mse_resid)\n",
        "\n",
        "print('\\nResults from sklearn:')\n",
        "print('Mean squared error: ',mse_ols_sklearn)\n",
        "print('Coefficient of determination: ',R2_ols_sklearn)\n",
        "\n",
        "# Check that the results match our \"res\" data frame in row \"numVar_12\", which is\n",
        "# a linear model with all explanatory variables\n",
        "print('\\nResults from row \"numVar_12\" from the \"res\" dataframe')\n",
        "print(res.loc['numVar_12',:])\n",
        "\n",
        "# => Note that in the simulation our X_train is a much smaller subsample \n",
        "#   of the full data set. Hence, we have some small variation but overall \n",
        "#   the result is very comparable "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI2i8omgj82L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now back to our simulation results. To inspect the overfitting of the model\n",
        "# lets plot R2 and MSE against the number of variables in the model.\n",
        "\n",
        "# Start with R2\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(res['Number of X Variables'],res['R2 train'],label='train')\n",
        "ax.plot(res['Number of X Variables'],res['R2 test'],label='test')\n",
        "ax.set_ylabel('R2')\n",
        "ax.set_xlabel('number of variables')\n",
        "ax.set_ylim(0,1)\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "ax.legend();\n",
        "\n",
        "# => Note that in same cases our R2 in the test set is negative, which means\n",
        "#    that our model is completely off..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpJGleUmtu-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ... and MSE\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(res['Number of X Variables'],res['MSE Train'],label='train')\n",
        "ax.plot(res['Number of X Variables'],res['MSE Test'],label='test')\n",
        "ax.set_ylabel('MSE')\n",
        "ax.set_xlabel('number of variables')\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "ax.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z-gJtvIejP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =====================================\n",
        "# Question to discuss in the group:\n",
        "# =====================================\n",
        "\n",
        "# What can you conclude from the plots?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX9o1ITDyx_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, we can find the \"best\" specification from our simulation \n",
        "# by checking where R2 is highest in test set\n",
        "iRes = res.loc[res['R2 test']==res['R2 test'].max(),:]\n",
        "lstFeatures[:int(iRes['Number of X Variables'])]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cAlNildepgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =====================================\n",
        "# Question to discuss in the group:\n",
        "# =====================================\n",
        "\n",
        "# Why might this model specification strategy not be ideal?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy4hWsgdQ6-f",
        "colab_type": "text"
      },
      "source": [
        "### Let's see how Lasso could be used here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoVC3W4BRq-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The idea is that instead of adding in one variable at a time, we use all\n",
        "# and let Lasso decide which ones to use. \n",
        "\n",
        "# It turns out that in order to get better results we need to make \n",
        "# the problem one step simpler for Lasso. If we use all polynomial terms \n",
        "# Lasso has convergence issues, at least with the restricted sample size \n",
        "# that we used above. Apparently the problem is that our variables are too \n",
        "# correlated. However, we can also illustrate the apporoach by using only linear \n",
        "# and square terms (excluding the interaction terms).  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTWCqeowEQ36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get only the linear and square terms from the polynomials\n",
        "idxSq = list(range(0,13))+[lstFeatures.index(c) for c in lstFeatures if '^2' in c]\n",
        "lstColNames = [lstFeatures[i] for i in idxSq]\n",
        "X_trainSq = X_train_poly[:,idxSq]\n",
        "X_testSq = X_test_poly[:,idxSq]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgptOE8SvOs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get actual names for features instead of x1,...,x11\n",
        "# this is required for a plot further down...\n",
        "sCols = pd.Series(lstColNames)\n",
        "sCols = sCols.replace({'\\^2':'_sq'},regex=True)\n",
        "sCols = sCols.replace({'1':'const'},regex=False)\n",
        "dctReplace = {f\"x{i}\":lstX[i] for i in range(10,len(lstX))}\n",
        "sCols = sCols.replace(dctReplace,regex=True)\n",
        "dctReplace = {f\"x{i}\":lstX[i] for i in range(0,10)}\n",
        "sCols = sCols.replace(dctReplace,regex=True)\n",
        "lstColNames = sCols\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMT0EcKm-49Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a list with alpha (= lambda in the lecture) values that we want to test for our Lasso estimation\n",
        "lstAlpha = np.logspace(-16, -9, num = 10, base = 2)\n",
        "lstAlpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqPfDYY-GtEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now loop over the alphas, run a Lasso estimation and get the model stats \n",
        "# Note the alphas here are just the same as the penalty (lambda)\n",
        "\n",
        "# Create a dataframe to hold model stats\n",
        "resLasso = pd.DataFrame()\n",
        "# Create a dataframe to hold coef\n",
        "resCoef = pd.DataFrame(columns=lstColNames)\n",
        "\n",
        "# Loop over a range of alpha value\n",
        "for alpha in lstAlpha:\n",
        "  print(alpha)\n",
        "  # Estimate Lasso\n",
        "  if alpha >0:\n",
        "    modLasso = Lasso(normalize=True, fit_intercept=True, alpha=alpha)\n",
        "  else:  \n",
        "    # in case of alpha = 0 use LinearRegression as recommended by sklearn\n",
        "    modLasso = LinearRegression(normalize=True, fit_intercept=True)\n",
        "  modLasso.fit(X_trainSq, Y_train)\n",
        "\n",
        "  # Get predicted values\n",
        "  Y_hat_train = modLasso.predict(X_trainSq)\n",
        "  Y_hat_test = modLasso.predict(X_testSq)\n",
        "\n",
        "  # Get model stats\n",
        "  resLasso.loc[f\"lasso_{alpha}\",'MSE Train'] = mean_squared_error(Y_train,Y_hat_train)\n",
        "  resLasso.loc[f\"lasso_{alpha}\",'MSE Test'] = mean_squared_error(Y_test,Y_hat_test)\n",
        "  # The coefficient of determination: 1 is perfect prediction\n",
        "  resLasso.loc[f\"lasso_{alpha}\",'R2 train'] = r2_score(Y_train,Y_hat_train)\n",
        "  resLasso.loc[f\"lasso_{alpha}\",'R2 test'] = r2_score(Y_test,Y_hat_test)\n",
        "  \n",
        "  resLasso.loc[f\"lasso_{alpha}\",'alpha'] = alpha\n",
        "\n",
        "  resCoef.loc[f'beta_hat_alpha{alpha}','alpha',] = alpha\n",
        "  resCoef.loc[f'beta_hat_alpha{alpha}',lstColNames] = modLasso.coef_.transpose()\n",
        "\n",
        "\n",
        "# => As you will see in the output, there are some values of alpha for which \n",
        "#   Lasso does not converge. This is not ideal but for our results we can \n",
        "#   ignore this for now.\n",
        "# => Again check how MSE and R2 developed for varying values of alpha\n",
        "\n",
        "resLasso"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1XJUewDK9Z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the \"best\" alpha by checking where MSE is lowest in test set \n",
        "# (or R2 is highest)\n",
        "iRes = resLasso.loc[resLasso['R2 test']==resLasso['R2 test'].max(),:]\n",
        "alphaBest = resLasso.loc[resLasso['R2 test']==resLasso['R2 test'].max(),'alpha'][0]\n",
        "print('The \"best\" (from those we tried in the simulations) is: ', alphaBest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEqYSwBvIDqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot result\n",
        "\n",
        "# Define range of Y that should be shown in plot\n",
        "rangeYLow = resLasso['MSE Train'].min()*0.99\n",
        "rangeYHigh = resLasso['MSE Test'].max()*1.01\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(resLasso['alpha'],resLasso['MSE Train'],label='train')\n",
        "ax.plot(resLasso['alpha'],resLasso['MSE Test'],label='test')\n",
        "ax.plot([alphaBest,alphaBest],[rangeYLow,rangeYHigh],label='\"best\" alpha',linestyle=':',color='black')\n",
        "ax.set_ylabel('MSE')\n",
        "ax.set_xlabel('alpha')\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "fig.autofmt_xdate(rotation=45)\n",
        "ax.set_ylim(rangeYLow,rangeYHigh)\n",
        "ax.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVGZFxZfpG6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Repeat the simulation with a slightly larger alpha range\n",
        "# in order to prepare a plot with the estimated coefficients against alpha \n",
        "# Df to hold coef\n",
        "resCoef = pd.DataFrame(columns=lstColNames)\n",
        "\n",
        "# Loop over a range of alpha value\n",
        "for alpha in np.logspace(-16, 0, num = 10, base = 2):\n",
        "  # Estimate Lasso\n",
        "  if alpha >0:\n",
        "    modLasso = Lasso(normalize=True, fit_intercept=True, alpha=alpha)\n",
        "  else:  \n",
        "    # in case of alpha = 0 use LinearRegression as recommended by sklearn\n",
        "    modLasso = LinearRegression(normalize=True, fit_intercept=True)\n",
        "  modLasso.fit(X_trainSq, Y_train)\n",
        "\n",
        "  resCoef.loc[f'beta_hat_alpha{alpha}','alpha',] = alpha\n",
        "  resCoef.loc[f'beta_hat_alpha{alpha}',lstColNames] = modLasso.coef_.transpose()\n",
        "\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "# Create traces\n",
        "fig = go.Figure()\n",
        "\n",
        "for strBeta in resCoef.columns[1:]:\n",
        "  # ax.plot(resCoef['alpha'],resPlot[strBeta],label=strBeta)\n",
        "  fig.add_trace(go.Scatter(x=resCoef['alpha'], y=resCoef[strBeta],\n",
        "                      mode='lines',\n",
        "                      name=strBeta))\n",
        "\n",
        "rangeYLow = resCoef.min().min()*0.99\n",
        "rangeYHigh = resCoef.max().max()*1.01\n",
        "fig.add_trace(go.Scatter(x=[alphaBest,alphaBest], y=[rangeYLow,rangeYHigh],\n",
        "                    mode='lines',\n",
        "                    name='bestAlpha',line=dict(color='black', dash='dash')))\n",
        "\n",
        "fig.update_layout(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    xaxis_type=\"log\",\n",
        "    xaxis = dict(\n",
        "       autorange='reversed'\n",
        "    ),\n",
        "    yaxis = dict(\n",
        "       range=[rangeYLow,rangeYHigh],\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZm0tXJ8DLDM",
        "colab_type": "text"
      },
      "source": [
        "# Day 1 Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czQzN5tNDZua",
        "colab_type": "text"
      },
      "source": [
        "Today's lab will have you run an OLS and LASSO regression using the deforestation data you saw in the introduction slides and in todays lecture (see code above). \n",
        "\n",
        "In the above specification we work with a rather restricted, preselected set of explnatory variables. Also we restricted the sample size artifically. Now try to run a model with a larger set of explanatory variables and using the full data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7l5mTf1zRwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you haven't already, load the deforestation data following the code above.\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9wzA0btgEL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define target (dependent) variable (% forest cover for 2018)\n",
        "strY = 'perc_treecover'\n",
        "\n",
        "# Define a list of features names (explantory variables)\n",
        "lstX = [\n",
        "  'bean',\n",
        " 'carrot',\n",
        " 'cassava',\n",
        " 'chickpea',\n",
        " 'citrus',\n",
        " 'coffee',\n",
        " 'groundnut',\n",
        " 'maize',\n",
        " 'soy',\n",
        " 'sugarcane',\n",
        " 'tomato',\n",
        " 'wheat',\n",
        " 'perm_water',\n",
        " 'travel_min',\n",
        " 'defor_2001',\n",
        " 'defor_2002',\n",
        " 'defor_2003',\n",
        " 'defor_2004',\n",
        " 'defor_2005',\n",
        " 'defor_2006',\n",
        " 'defor_2007',\n",
        " 'defor_2008',\n",
        " 'defor_2009',\n",
        " 'defor_2010',\n",
        " 'defor_2011',\n",
        " 'defor_2012',\n",
        " 'defor_2013',\n",
        " 'defor_2014',\n",
        " 'defor_2015',\n",
        " 'defor_2016',\n",
        " 'defor_2017',\n",
        " 'defor_2018',\n",
        " 'wdpa_1990',\n",
        " 'wdpa_1991',\n",
        " 'wdpa_1993',\n",
        " 'wdpa_1994',\n",
        " 'wdpa_1995',\n",
        " 'wdpa_1996',\n",
        " 'wdpa_1997',\n",
        " 'wdpa_1998',\n",
        " 'wdpa_1999',\n",
        " 'wdpa_2000',\n",
        " 'wdpa_2001',\n",
        " 'wdpa_2002',\n",
        " 'wdpa_2003',\n",
        " 'wdpa_2004',\n",
        " 'wdpa_2005',\n",
        " 'wdpa_2006',\n",
        " 'wdpa_2007',\n",
        " 'wdpa_2008',\n",
        " 'wdpa_2009',\n",
        " 'wdpa_2010',\n",
        " 'wdpa_2011',\n",
        " 'wdpa_2012',\n",
        " 'wdpa_2014',\n",
        " 'wdpa_2015',\n",
        " 'wdpa_2017',\n",
        " 'wdpa_2018',\n",
        " 'chirps_2001',\n",
        " 'chirps_2002',\n",
        " 'chirps_2003',\n",
        " 'chirps_2004',\n",
        " 'chirps_2005',\n",
        " 'chirps_2006',\n",
        " 'chirps_2007',\n",
        " 'chirps_2008',\n",
        " 'chirps_2009',\n",
        " 'chirps_2010',\n",
        " 'chirps_2011',\n",
        " 'chirps_2012',\n",
        " 'chirps_2013',\n",
        " 'chirps_2014',\n",
        " 'chirps_2015',\n",
        " 'chirps_2016',\n",
        " 'chirps_2017',\n",
        " 'chirps_2018',\n",
        " 'population_2000',\n",
        " 'population_2005',\n",
        " 'population_2010',\n",
        " 'population_2015',\n",
        " 'cropland',\n",
        " 'pasture',\n",
        " 'mean_elev',\n",
        " 'sd_elev',\n",
        " 'near_road',\n",
        " 'bean_lag_1st_order',\n",
        " 'carrot_lag_1st_order',\n",
        " 'cassava_lag_1st_order',\n",
        " 'chickpea_lag_1st_order',\n",
        " 'citrus_lag_1st_order',\n",
        " 'coffee_lag_1st_order',\n",
        " 'groundnut_lag_1st_order',\n",
        " 'maize_lag_1st_order',\n",
        " 'soy_lag_1st_order',\n",
        " 'sugarcane_lag_1st_order',\n",
        " 'tomato_lag_1st_order',\n",
        " 'wheat_lag_1st_order',\n",
        " 'perm_water_lag_1st_order',\n",
        " 'travel_min_lag_1st_order',\n",
        " 'defor_2001_lag_1st_order',\n",
        " 'defor_2002_lag_1st_order',\n",
        " 'defor_2003_lag_1st_order',\n",
        " 'defor_2004_lag_1st_order',\n",
        " 'defor_2005_lag_1st_order',\n",
        " 'defor_2006_lag_1st_order',\n",
        " 'defor_2007_lag_1st_order',\n",
        " 'defor_2008_lag_1st_order',\n",
        " 'defor_2009_lag_1st_order',\n",
        " 'defor_2010_lag_1st_order',\n",
        " 'defor_2011_lag_1st_order',\n",
        " 'defor_2012_lag_1st_order',\n",
        " 'defor_2013_lag_1st_order',\n",
        " 'defor_2014_lag_1st_order',\n",
        " 'defor_2015_lag_1st_order',\n",
        " 'defor_2016_lag_1st_order',\n",
        " 'defor_2017_lag_1st_order',\n",
        " 'defor_2018_lag_1st_order',\n",
        " 'wdpa_1990_lag_1st_order',\n",
        " 'wdpa_1991_lag_1st_order',\n",
        " 'wdpa_1993_lag_1st_order',\n",
        " 'wdpa_1994_lag_1st_order',\n",
        " 'wdpa_1995_lag_1st_order',\n",
        " 'wdpa_1996_lag_1st_order',\n",
        " 'wdpa_1997_lag_1st_order',\n",
        " 'wdpa_1998_lag_1st_order',\n",
        " 'wdpa_1999_lag_1st_order',\n",
        " 'wdpa_2000_lag_1st_order',\n",
        " 'wdpa_2001_lag_1st_order',\n",
        " 'wdpa_2002_lag_1st_order',\n",
        " 'wdpa_2003_lag_1st_order',\n",
        " 'wdpa_2004_lag_1st_order',\n",
        " 'wdpa_2005_lag_1st_order',\n",
        " 'wdpa_2006_lag_1st_order',\n",
        " 'wdpa_2007_lag_1st_order',\n",
        " 'wdpa_2008_lag_1st_order',\n",
        " 'wdpa_2009_lag_1st_order',\n",
        " 'wdpa_2010_lag_1st_order',\n",
        " 'wdpa_2011_lag_1st_order',\n",
        " 'wdpa_2012_lag_1st_order',\n",
        " 'wdpa_2014_lag_1st_order',\n",
        " 'wdpa_2015_lag_1st_order',\n",
        " 'wdpa_2017_lag_1st_order',\n",
        " 'wdpa_2018_lag_1st_order',\n",
        " 'chirps_2001_lag_1st_order',\n",
        " 'chirps_2002_lag_1st_order',\n",
        " 'chirps_2003_lag_1st_order',\n",
        " 'chirps_2004_lag_1st_order',\n",
        " 'chirps_2005_lag_1st_order',\n",
        " 'chirps_2006_lag_1st_order',\n",
        " 'chirps_2007_lag_1st_order',\n",
        " 'chirps_2008_lag_1st_order',\n",
        " 'chirps_2009_lag_1st_order',\n",
        " 'chirps_2010_lag_1st_order',\n",
        " 'chirps_2011_lag_1st_order',\n",
        " 'chirps_2012_lag_1st_order',\n",
        " 'chirps_2013_lag_1st_order',\n",
        " 'chirps_2014_lag_1st_order',\n",
        " 'chirps_2015_lag_1st_order',\n",
        " 'chirps_2016_lag_1st_order',\n",
        " 'chirps_2017_lag_1st_order',\n",
        " 'chirps_2018_lag_1st_order',\n",
        " 'population_2000_lag_1st_order',\n",
        " 'population_2005_lag_1st_order',\n",
        " 'population_2010_lag_1st_order',\n",
        " 'population_2015_lag_1st_order',\n",
        " 'cropland_lag_1st_order',\n",
        " 'pasture_lag_1st_order',\n",
        " 'mean_elev_lag_1st_order',\n",
        " 'sd_elev_lag_1st_order',\n",
        " 'near_road_lag_1st_order',\n",
        " ]\n",
        "\n",
        "# Get target variable and features\n",
        "Y_all = df[strY]\n",
        "X_all = df.loc[:,lstX]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXKxlWM3DQ6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data into a train and test set as above\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5QKVCXzgbxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run an OLS model to predict percent forest cover (using the 'train' part of the previous split for the training)\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuYe3OOhgnh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate the predicted values for forest cover in the train and test set\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqzGowruhSHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Produce measures of model fit (R2 and MSE) for train and test data\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrFTs0h0EIey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot predicted versus actual forest cover for test set\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYkF2bMzGbrA",
        "colab_type": "text"
      },
      "source": [
        "Now run the same specification using a LASSO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbAtwaimEVlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a LASSO function from sklearn\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n",
        "modLasso = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8Pkscz1EqfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the same variable specification as for you OLS model and fit it \n",
        "# to the training data\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GJ98XMhjNLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create fitted values for train and test set and compare model fit (R2/MSE)\n",
        "# as you have done it for OLS\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GITrlka73R1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check how many coefficients are selected in Lasso (i.e. are greater 0)\n",
        "print('Number of total coefficients in Lasso: ' , modLasso.coef_.shape[0])\n",
        "print('Number of selected coefficients in Lasso: ' ,np.sum(modLasso.coef_>0))\n",
        "\n",
        "print('\\nList selected variables:\\n')\n",
        "list(pd.Series(lstX).loc[modLasso.coef_>0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZR-HB4KFLXb",
        "colab_type": "text"
      },
      "source": [
        "Now change your alpha (lambda) and repeat.  \n",
        "\n",
        "What happens as you increase your alphs? As you decrease your alphs? Are the results more or less similar to your OLS results?\n",
        "\n",
        "Note: In this specific case it might be quite difficult to beat the simple OLS result in terms of MSE/R2 in the test set, because OLS is not really overfitting in this example.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBE-pVQGFUwE",
        "colab_type": "text"
      },
      "source": [
        "(Optional) So far we are just choosing our training and test set randomly.  Is this valid if there is high spatial correlation?  (if you like) try splitting your data based on latitude and re-run the OLS and the LASSO.  Do you expect your results to be better or worse than using the random split?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uvIS2VDE0KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set a cut-point in the latitude variable to split data into a train and test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRicA12RICPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# re-run OLS and LASSO models using this new train and test set\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}