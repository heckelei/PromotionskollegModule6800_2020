{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_Day3_4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3dbMPsC8kYb",
        "colab_type": "text"
      },
      "source": [
        "# Day 3/4: Code used during lecture and lab assignment\n",
        "\n",
        "##Instructions\n",
        "\n",
        "The notebook combines 'code used during lecture' with the corresponding lab assignment (see further down)\n",
        "The lab assignment can be done largely by copying/paste/modification of the code used during the lecture\n",
        "Please add answers/discussion/comments to the notebook as comments or text box. Do not create another file in addition.\n",
        "When you are done with your assignment, save the notebook in drive and add your last name to the name of the file\n",
        "Upload your final notebook to https://uni-bonn.sciebo.de/s/mTpqLLBN9Wu71Ku at the end of the day. The password for access is the same as before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku0lCEownFJh",
        "colab_type": "text"
      },
      "source": [
        "### Notes:\n",
        "- The first part of the notebook contains code from the shapeley value lecture (day 3), the second part comprises all the code on NN (days 3 and 4),  \n",
        "- The intention of the NN part of the notebook is somewhat different from the notebooks for the other days. Here, we do not aim for fitting an optimal model. The steps that we take here are not an illustration of how you would actually approach an estimation task. Instead we want to play around with a NN in serveral ways to build your understanding of how NN work.\n",
        "- We begin with a NN used for autoencoding and then move towards using a NN for prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQLmu7uoNE7h",
        "colab_type": "text"
      },
      "source": [
        "### Code used during lecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QadP7iYo9N4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sc\n",
        "import datetime, os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "# Set the numpy random seed\n",
        "np.random.seed(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEfL_HEFkr02",
        "colab_type": "text"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcF4Tis482HQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download data \n",
        "!wget http://www.ilr.uni-bonn.de/agpo/courses/ml/brazil_all_data_v2.gz\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjyhe3gvEvUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data into dataframe\n",
        "df = pd.read_parquet('brazil_all_data_v2.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBWEGCmmkyRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define targets and features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAmje5rJAj9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define binary variable for deforestration in 2018 as in lab day 2\n",
        "df['D_defor_2018'] = df['defor_2018']>0\n",
        "Y_all = df['D_defor_2018']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zieYJbMh-mLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Prepare the data \n",
        "# list(df.columns[~df.columns.str.contains('2018')])\n",
        "# Variables from day 2\n",
        "lstX = [\n",
        "  'wdpa_2017',\n",
        "  'population_2015',\n",
        "  'chirps_2017',\n",
        "  'defor_2017',\n",
        "  'maize',\n",
        "  'soy',\n",
        "  'sugarcane',\n",
        "  'perc_treecover',\n",
        "  'perm_water',\n",
        "  'travel_min',\n",
        "  'cropland',\n",
        "  'mean_elev',\n",
        "  'sd_elev',\n",
        "  'near_road',\n",
        "  'defor_2017_lag_1st_order',\n",
        "  'wdpa_2017_lag_1st_order',\n",
        "  'chirps_2017_lag_1st_order',\n",
        "  'population_2015_lag_1st_order',\n",
        "  'maize_lag_1st_order',\n",
        "  'soy_lag_1st_order',\n",
        "  'sugarcane_lag_1st_order',\n",
        "  'perc_treecover_lag_1st_order',\n",
        "  'perm_water_lag_1st_order',\n",
        "  'travel_min_lag_1st_order',\n",
        "  'cropland_lag_1st_order',\n",
        "  'mean_elev_lag_1st_order',\n",
        "  'sd_elev_lag_1st_order',\n",
        "  'near_road_lag_1st_order',\n",
        " ]\n",
        "\n",
        "X_all = df.loc[:,lstX]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsETnLYl_QP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_all.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGbT8-7kAfIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into train and test data using sklearn train_test_split object\n",
        "#   (see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "\n",
        "#   Note: This randomly split the data in 80% train and 20% test data\n",
        "X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(X_all, Y_all, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msCExuk8A0QF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MinMax scale data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scalerX = MinMaxScaler()\n",
        "X_train = scalerX.fit_transform(X_train_raw)\n",
        "X_test = scalerX.transform(X_test_raw)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS0JSHtcmWGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define helper function that we will use below to print the stats of each model\n",
        "def printOutput(Y_score,Y_true):\n",
        "\n",
        "  # Get true positive and false positive rate\n",
        "  # See: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
        "  fpr_Lg, tpr_Lg, _ = roc_curve(Y_true, Y_score)\n",
        "\n",
        "  # Get the Area under the cureve (AUC)\n",
        "  # See: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
        "  roc_auc_Lg = auc(fpr_Lg, tpr_Lg)\n",
        "\n",
        "  print('\\nROC AUC', roc_auc_Lg)\n",
        "\n",
        "  # Plot the ROC curve\n",
        "  plt.figure()\n",
        "  lw = 2\n",
        "  plt.plot(fpr_Lg, tpr_Lg, color='darkorange',\n",
        "          lw=lw, label='ROC curve (area = %0.2f' % roc_auc_Lg)\n",
        "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Receiver operating characteristic example')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch0YN9iV-4b-",
        "colab_type": "text"
      },
      "source": [
        "# **Part One: SHAP Values**\n",
        "The first part of the lab focus on SHAP values. First we show you how to plot SHAP values for the XG Boost model, which you have already seen in the lecutre and the lab session in the previous day. Then you  should create SHAP values for the logit model and explore how SHAP value would look like in a well known linear model.\n",
        "\n",
        "More on SHAP values:\n",
        "\n",
        "https://github.com/slundberg/shap\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_DUeyxqAPNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit a logistic regression model using sklearn \n",
        "# (see: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "\n",
        "# Create the model object\n",
        "modelLg = LogisticRegression(random_state=0,penalty='none',fit_intercept=True,max_iter=1000)\n",
        "# Fit the model using the training data\n",
        "modelLg.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS9tcR5WAQWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "# Fit a decision tree using sklearn\n",
        "# (see https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
        "\n",
        "# Define a model object\n",
        "modelTree = tree.DecisionTreeClassifier()\n",
        "# Fit the model\n",
        "modelTree = modelTree.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FEJk2_WATC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run a random forest using sklearn and default hyperparameters (note, this will take a few minutes)\n",
        "# (see https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create model object\n",
        "modelForest = RandomForestClassifier()\n",
        "# Fit model\n",
        "modelForest = modelForest.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1OZ5dCyAYSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now run an XGBoost model for the same task \n",
        "import xgboost as xgb\n",
        "model_xgb = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
        "\n",
        "# Fit model to data\n",
        "model_xgb.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G_HeQtnAcFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First install the SHAP libary\n",
        "!pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTE1zcqWBby0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the shape libary \n",
        "import shap\n",
        "# Load JS visualization code to notebook\n",
        "shap.initjs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yxB5jDGBcnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataframe for our train data that includes the variable names\n",
        "df_X_train = pd.DataFrame(X_train,columns=lstX)\n",
        "\n",
        "# Explain the model's predictions using SHAP\n",
        "# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\n",
        "explainer = shap.TreeExplainer(model_xgb)\n",
        "\n",
        "# Calculate the shape values using the TreeExplainer object\n",
        "shap_values = explainer.shap_values(df_X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iBS2rRuBjfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the first prediction's explanation\n",
        "# shap.force_plot(explainer.expected_value, shap_values[0,:], df_X_train.iloc[0,:])\n",
        "\n",
        "# If you have a javascript error use matplotlib=True to avoid Javascript\n",
        "shap.force_plot(explainer.expected_value, shap_values[0,:], df_X_train.iloc[0,:],matplotlib=True)\n",
        "\n",
        "# Note: This might look different than the version the slides because another random seed\n",
        "#       was used to create the plots in the slides"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHA3Lh-ZBqmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ====================\n",
        "# Discuss in the group\n",
        "# ====================\n",
        "# How do you interpret this plot?\n",
        "# When might you want to use a plot like this?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYxvmFWNBriW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize the effects of all the features\n",
        "shap.summary_plot(shap_values, df_X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhTi4m7YB0K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# ====================\n",
        "# Discuss in the group\n",
        "# ====================\n",
        "# How do you interpret this plot?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5QIdtGsB6RB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# create a dependence plot to show the effect of a single feature across the whole dataset\n",
        "shap.dependence_plot(\"defor_2017\", shap_values, df_X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDA1xZNEB7MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ====================\n",
        "# Discuss in the group\n",
        "# ====================\n",
        "# How do you interpret this plot?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-olD0n8B_wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ====================\n",
        "# Discuss in the group\n",
        "# ====================\n",
        "# Before you start creating the plots, discuss in the group what kind of \n",
        "# results you expect for the linear model. Specifically, think about how the \n",
        "# last plot (the dependence_plot) would look in this case."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E25A2Sg_CEDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we use the shap.LinearExplainer() function instead of the\n",
        "# shap.TreeExplainer(...) we used above\n",
        " \n",
        "# You can have a look here for a reference:\n",
        "# https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html\n",
        "\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n",
        "explainer = ...\n",
        "\n",
        "shap_values = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CDCjFiICHL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now create a dependence plot to show the effect of a single feature \n",
        "# across the whole dataset, as was done above but now for the logit model\n",
        "\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n",
        " ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bej8MxAfCUa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ====================\n",
        "# Discuss in the group\n",
        "# ====================\n",
        "\n",
        "\n",
        "# 1) Does this look like to what you expected?\n",
        "\n",
        "# 2) How does this compare to the plot for the XGB model. What can you conclude?\n",
        "\n",
        "# Note: Below you find the usual regression output for logit model again. This \n",
        "#       might be interesting as a reference."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp2EXrOJCWu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Have a look at the SHAP summary plot\n",
        "shap.summary_plot(shap_values, df_X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsu6_a-sCctM",
        "colab_type": "text"
      },
      "source": [
        "## **For your reference**\n",
        "\n",
        "Lets create our usual regression output for the logit model as a reference\n",
        "\n",
        "This used the same code from the lab intro session.\n",
        "\n",
        "(Not need to change/do anything here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaC5K-uQCZV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import norm\n",
        "# Function to calculate pvalues and standard errors for a scikit-learn logisticRegression\n",
        "# Source: https://stackoverflow.com/questions/25122999/scikit-learn-how-to-check-coefficients-significance\n",
        "def logit_pvalue(model, x):\n",
        "    \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
        "    parameters:\n",
        "        model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
        "        x:     matrix on which the model was fit\n",
        "    This function uses asymtptics for maximum likelihood estimates.\n",
        "    \"\"\"\n",
        "    p = model.predict_proba(x)\n",
        "    n = len(p)\n",
        "    m = len(model.coef_[0]) + 1\n",
        "    # m = len(model.coef_[0])\n",
        "    # coefs = model.coef_[0]\n",
        "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
        "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
        "    ans = np.zeros((m, m))\n",
        "    for i in range(n):\n",
        "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
        "    vcov = np.linalg.inv(np.matrix(ans))\n",
        "    se = np.sqrt(np.diag(vcov))\n",
        "    t =  coefs/se  \n",
        "    p = (1 - norm.cdf(abs(t))) * 2\n",
        "    return se, p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_UOyyFCCqe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the previously created function to create a regression output table\n",
        "se, p = logit_pvalue(modelLg, X_train)\n",
        "coefs = np.concatenate([modelLg.intercept_, modelLg.coef_[0]]).T\n",
        "resCoef = pd.DataFrame(coefs,index=['constant']+lstX)\n",
        "resCoef.columns = ['coef']\n",
        "resCoef['se'] = se\n",
        "resCoef['pval'] = p\n",
        "resCoef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug0JyzwZC1hz",
        "colab_type": "text"
      },
      "source": [
        "### **Optional Tasks**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0MHR3oxoIB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Optional-1) Explore the Shapley Value Explanations for different sub-sets of the data (e.g. protected areas versus others) \n",
        "#  and in a few sentences, discuss your findings\n",
        "#================\n",
        "\n",
        "#================"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLGC0nNYDbiw",
        "colab_type": "text"
      },
      "source": [
        "# **Part Two : NN** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQGV2AW0Z-Us",
        "colab_type": "text"
      },
      "source": [
        "# Outline for NN part\n",
        "\n",
        "\n",
        "- Start with PCA to do dimensionality reduction. Use the encoded data to run\n",
        "  a logistic regression \n",
        "- Train a autoencoder\n",
        "- Build an encoder/decoder model from the autoencoder\n",
        "- Use the encoder to encode the test data and use the decoder to transform \n",
        "  it back to the original space => See how much information is lost\n",
        "- Use the encoder to encode the data from the input space in a lower dim \n",
        "  encoding space. Use this encoding to run a logistic regression \n",
        "  (similarly as with the PCA)\n",
        "- Lets do exaclty the same thing but now using a NN setting where we use the \n",
        "  autoencoder as a pretrained first layer and we train only the output layer\n",
        "  (this is basically the same as in the step befor)\n",
        "- *Now lets do everything in one step. Let the NN train all parameters, \n",
        "  either completly end-to-end or using the encoder layer from the autoencoder \n",
        "  as a starting point*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkd4LHKlPFoX",
        "colab_type": "text"
      },
      "source": [
        "# Step 1: Illustration Principal Component Analysis (PCA) \n",
        "Before looking into neural networks lets see how we could use Principal Component Analysis for our forest data.\n",
        "\n",
        "We will not look into the detailes of PCA; the only important point is to understand that PCA is a dimensionality reduction technique that transforms data from a higher dimensional space to a lower dimensional space while trying to limit the loss of information. \n",
        "\n",
        "Note: Above we defined an input feature matrix with 28 veriables (features) and on day 2 you saw that it is perfectly possible\\feasible to run a logisitic regression with all 28 variables. Hence a dimensionality reduction is not necessary here. Nevertheless, we use this example in order to illustrate how PCA and then following NN autoencoding work. A real world setup where such a dimenstionality reduction approach makes actual sense would involve a sgnificantly larger number of variables k.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eV7iV0P_BzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the numpy random seed\n",
        "np.random.seed(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F-B2oMeNn0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit an PCA\n",
        "encoding_dim = 8 # map input data to a 8 dimensional space\n",
        "pca = PCA(n_components=encoding_dim)\n",
        "pca.fit(X_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plHWZmSBPTeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we can use the trained PCA to encode our input data in the \n",
        "# lower dimensional space \n",
        "encode_PCA = pca.transform(X_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4BRmQf7uUCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Dim of feature matrix',X_train.shape)\n",
        "print('Dim of encoding matrix',encode_PCA.shape)\n",
        "print( 'Explained variance ratio', pca.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fFTrXGfOduk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can use this lower dimensional encoding in a logistic regression \n",
        "# This means our logistic regression now does not have 28 dimensions as our\n",
        "# features matrix but only 8 dimensions (=encoding_dim)  \n",
        "\n",
        "# Fit a logistic regression using the PCA encoding\n",
        "# Create the model object\n",
        "modelLgPCA = LogisticRegression(random_state=0,penalty='none',fit_intercept=True, max_iter=1000)\n",
        "# Fit the model using the training data\n",
        "modelLgPCA.fit(encode_PCA, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00yWK21Emvw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the predicted probabiltities \n",
        "Y_score = modelLgPCA.decision_function(pca.transform(X_test))\n",
        "\n",
        "printOutput(Y_score,Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTIcPyu0m6As",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you compare this result with the logit regression from day 2 using all \n",
        "# features you can see that this is clearly worse. However, it is still a model \n",
        "# is not completly uselss... so it seems that out 8-dimensional encoding \n",
        "# contains something usefull.   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOhYLk37pzS2",
        "colab_type": "text"
      },
      "source": [
        "# Step 2: Lets see how we can achive the same using an Autoencoder\n",
        "\n",
        "For etstimating NN we use the libary \"Keras\" which is one of the most popular deep learning libaries (https://keras.io/). Keras is a wrapper for Tensorflow (https://www.tensorflow.org/) which allows to specify a tensorflow model in few commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqXR-3mdumyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load keras and tesorflow \n",
        "import tensorflow as tf\n",
        "import keras as ke\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx7eT-2x-p_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Setup Autoencode as an Keras model\n",
        "# This example is adopted based on: https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "\n",
        "# Set dimensionality of the encoding space. \n",
        "# As in the PCA example we want a 8 dim encoding space.\n",
        "encoding_dim = 8\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "print('Encoding Dim is equal to:', encoding_dim)\n",
        "print('Input Dim is equal to:', input_dim)\n",
        "\n",
        "# In Keras we can specify a NN layer in one line of code. This is what \n",
        "# we use here to specify an...\n",
        "\n",
        "# 1) input layer that has the dimension equal to the number of variables\n",
        "#    in the input (here =28)\n",
        "input_dat = Input(shape=(input_dim,))\n",
        "\n",
        "# 2) the second layer is the encoding layer. It takes the output from the\n",
        "#    input layer (\"input_dat\") as input and is a \"dense\" layer with \n",
        "#    \"encoding_dim\" (=8) neurons. It uses the \"relu\" as activation \n",
        "encoded = Dense(encoding_dim, activation='relu')(input_dat)\n",
        "\n",
        "# 3) The third layer is the decoding layer, which is our output layer. \n",
        "#    It takes the output from the encoding layer as input (\"encoded\"). It has \n",
        "#    \"input_dim\"=28 neurons. The acitvation function is not specified here, which \n",
        "#   means that we use the default activation function [identity function]. \n",
        "decoded = Dense(input_dim)(encoded) \n",
        "\n",
        "# Using these layer we build a NN in tensorflow. We do this be passing the \n",
        "# input layer and the last layer to the keras \"Model\" function. \n",
        "# This is sufficient for Keras to now how to build the complete model. \n",
        "# (The information how the hidden layers should look like, is know because \n",
        "# we passed this as input to the output layer)\n",
        "autoencoder = Model(input_dat, decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukZfiU1DL2iG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Have a look how the model looks like\n",
        "# Our input layer does not have any paramters. This is simply a placeholder \n",
        "# for our data that we will pass to the model.\n",
        "# The encoding layer has 232 neuros = 28 (input) x 8 (output) + 8 (constants)\n",
        "# The decoding layer has 252 neuros = 8 (input) x 28 (output) + 28 (constants)\n",
        "# In total we therefore have 484 parameters. Already quite a lot for such a \n",
        "# small model!\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpNF6tHUKLag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we tell Keras/tensorflow which optimization algorithm we want to use\n",
        "# We also need to define a learning rate. This might not be the optimal choice\n",
        "# here. It we could tune this parameter to obtain better results\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n",
        "\n",
        "# We also need to define the type of loss function we would to considere. \n",
        "# Since we have a regression task we use MSE. \n",
        "# With this we can now compile the model. \n",
        "autoencoder.compile(optimizer=optimizer, loss='MeanSquaredError')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKyhh5IyAX0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally we can start training our Auotencoder using the \"fit()\" function\n",
        "# - we pass the training data (X_train, X_train)\n",
        "# - we specify the number fo epochs, how often we want to move thorugh \n",
        "#   the data for training\n",
        "# - we specify the size of the minibatches \n",
        "# - we specify that we want the shuffle the data such that the order is changed \n",
        "#   in each epoch\n",
        "# - we also pass the test data for validation\n",
        "history = autoencoder.fit(\n",
        "                X_train, X_train, \n",
        "                epochs=300,        \n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test),\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGgxfoZnq8Oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let plot the Traning and validation Loss in order to see if our model \n",
        "# overfitts\n",
        "plt.plot(range(1, len(history.history['loss']) + 1), history.history['loss'],'r', label='Training Loss')\n",
        "plt.plot(range(1, len(history.history['val_loss']) + 1), history.history['val_loss'],'b', label='Validation Loss')\n",
        "plt.title('Training and validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5--WdpPIxP_",
        "colab_type": "text"
      },
      "source": [
        "# Step 3a: Use the trained autoencoder to build an encoder and decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y1m9gM5Bch3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we can use the trained encoder/decoder layer of our autoencoder\n",
        "# and build seperated encoder and decoder networks\n",
        "\n",
        "\n",
        "# Using only the first layer of the Autoencoder we can build an encoder model\n",
        "# This model maps an input to its encoded representation\n",
        "# Use again the Keras \"Model\" function passin again the input layer and now \n",
        "# the encoder layer as a output (note that by now the encoder layer is trained)\n",
        "encoder = Model(input_dat, encoded)\n",
        " \n",
        "# Have a look how the model looks like\n",
        "# This is basically the first halb of our autoencoder from above\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGl3djoDCD0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Additionally lets build a decoder model that can take an encoded input\n",
        "# and outputs the original variables \n",
        "\n",
        "# create a placeholder for an the encoded input with dimension \n",
        "# equal to the encoding dimension\n",
        "encoded_input = Input(shape=(encoding_dim,))\n",
        "\n",
        "# Get the last layer of the autoencoder model \n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "\n",
        "# create the decoder model using the last layer \n",
        "# (the output layer of the autoencoder)\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "# Have a look how the model looks like\n",
        "# This is basically the second half of our autoencoder\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQAjp3sMCMbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets see what we can do with these two models...\n",
        "# Lets look at one specific observation (the first observation)\n",
        "encoded_dat = encoder.predict(X_test[[0],:])\n",
        "# This is the encoded data for the first observation (with 8 dimensions)\n",
        "# This does not really have an interpreation but we can see how much\n",
        "# information is in this encoding by using it to reconstruct the \n",
        "# original values using our decoder... (see next cell)\n",
        "encoded_dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpOsrNmDCRBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the first observation after encoding and then decoding again.\n",
        "# (compare this to the orignal values, next cell...)\n",
        "decoded_dat = decoder.predict(encoded_dat)\n",
        "decoded_dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nq3hzxuCg4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This are the original values of the first observation\n",
        "# This does not look to bad!\n",
        "X_test[[0],:]\n",
        "\n",
        "# Make sure that you understand what is happening here. This shows that to some \n",
        "# extent we are able to compress the information in the 28dim input vector in \n",
        "# something 8dim and then reconstruct the orginal 28dim rather closely. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU7vQl4lCiOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets plot a scatter between original input and the \n",
        "# \"predicted\" input after the encoding/decoding steo \n",
        "\n",
        "# If the encoded/decoding steps would work perfectly you would get a \n",
        "# diagonal line.   \n",
        "iCol = 5 # change this to values between (0-27) to plot other variables\n",
        "encoded_dat = encoder.predict(X_test)\n",
        "decoded_dat = decoder.predict(encoded_dat)\n",
        "plt.scatter(X_test[:,iCol],decoded_dat[:,iCol])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aJz0nMeGthf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets compute the R\n",
        "R2_test = r2_score(X_test,decoded_dat)\n",
        "R2_test\n",
        "# Ideally we would have a \"1\" which would mean that we have not information \n",
        "# loss trough our encoding/decoding step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrScCLeSh5PJ",
        "colab_type": "text"
      },
      "source": [
        "# Step 3b: Estimating a simple logistic regression model using the encoded data from the Autoencoder as explantory variables\n",
        "\n",
        "Similarly as with PCA lets use the encoding and run a logit model to predict deforestation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYuofe_gOlJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Similarly as in the PCA example we now use the encoded data \n",
        "# as an input for a logistic regression. \n",
        "encoded_dat = encoder.predict(X_train)\n",
        "\n",
        "# Fit a logistic regression \n",
        "modelLg = LogisticRegression(random_state=0,penalty='none',fit_intercept=True, max_iter=1000)\n",
        "# Fit the model using the training data\n",
        "modelLg.fit(encoded_dat, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sHOBhpzrwju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the predicted probabiltities \n",
        "Y_score = modelLg.decision_function(encoder.predict(X_test))\n",
        "\n",
        "printOutput(Y_score,Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQgJIAiYiTwf",
        "colab_type": "text"
      },
      "source": [
        "# Do the same thing but now as a NN specification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7lRymfe2qOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can do the same thing but now defining this as a NN. For this we \n",
        "# define a output layer using a sigmoid activation function. During \n",
        "# traning we freez the encoding layers and only train the last layer. \n",
        "# Methodically this is exactly the same thing as running a logit model \n",
        "# using the encodings. The only thing that is different is the way we implement. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn9Y83IBieGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is the size of our encoded representations\n",
        "encoding_dim = 8\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "print('Encoding Dim is equal to:', encoding_dim)\n",
        "print('Input Dim is equal to:', input_dim)\n",
        "\n",
        "# Specify an output layer that uses our encoded layer from above. And has a \n",
        "# sigmoid activation. This is exactly equal then running a logit model \n",
        "# using the encoded input as we did above. \n",
        "output = Dense(1, activation='sigmoid')(encoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "NN = Model(input_dat, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAwpLlGJk-Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze the encoding layer such that only the output layer is trained\n",
        "NN.layers[1].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb3rSsSDtK4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Have a look how the model looks like\n",
        "# - The hidden layer is our encoding layer from above with 232 (those weights\n",
        "#   we froze such that they are not trainable)\n",
        "# - Our last layer has only 9 parameters, this is a logit model with 8 \n",
        "#   explantory variables (our encoding) and a constant\n",
        "NN.summary()\n",
        "\n",
        "# Make sure you understand the parallels to the logit model above. \n",
        "# This should illustrate that you can interprete a NN (and this holds of \n",
        "# basically all NN) as a model that does an encoding in all the layers up to \n",
        "# the last and then in the last layer a logit model takes this encoding and \n",
        "# maps it to the output!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGTWt9d7i-xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile model, this time using an other optimizer and \"crossentropy\" as the \n",
        "# Loss function\n",
        "# Similarly as above, we normally would tune the setting here\n",
        "NN.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AFcKdFgxtqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the tensorboard extension \n",
        "# tensorbord enables tracking experiment metrics like loss and accuracy, visualizing the model graph---\n",
        "%load_ext tensorboard\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPJGJnGg0BXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C09EJgCezsvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspect and understand your model runs and graphs with TensorBoard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSuBeV-JjS13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train only the last layer, i.e. fit the logit model\n",
        "from keras.callbacks import TensorBoard\n",
        "import datetime, os\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "history = NN.fit(X_train, Y_train,\n",
        "                epochs=150,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, Y_test),\n",
        "                callbacks=[tensorboard_callback]\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUYHa9r_sKfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the predicted probabiltities \n",
        "Y_score_NN = NN.predict(X_test).ravel()\n",
        "\n",
        "printOutput(Y_score_NN,Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djRFBq5LDWyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To rather \"proof\" that the two aproaches are ideed the same \n",
        "# lets compare the coefficients estimated in the logit model...\n",
        "modelLg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7V3iSqTDl7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ... with the weights estimated in the last layer of the NN\n",
        "NN.layers[-1].weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywX3cj1PsXtC",
        "colab_type": "text"
      },
      "source": [
        "# Lab NN: Train an NN end-to-end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnwZdwgBEiUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not lets setup a NN that directly takes the input and predicts deforestations.\n",
        "# Lets train this NN end-to-end. \n",
        "\n",
        "# Define you NN \n",
        "# Note: All the parts you need to solve this task are above. You only need \n",
        "# to copy the right pices from above.  \n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n",
        "\n",
        "...\n",
        "\n",
        "NN = Model( .. , ... )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r71tIhl2EmkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile model\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n",
        "NN.compile(...)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn-KFBryEprT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model \n",
        "\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n",
        "history = NN.fit(...)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AiWOAo-s2Ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the predicted probabiltities \n",
        "Y_score_NN = NN.predict(X_test).ravel()\n",
        "\n",
        "printOutput(Y_score_NN,Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R8E62YzmXc8",
        "colab_type": "text"
      },
      "source": [
        "# Lab NN: Optional task\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqd5UCdTNBCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In the lecture and in the code above we only consider \"undercomplete\" \n",
        "# Autoencoder, where the hidden layer has fewer layers as the input.\n",
        "# We can also train Autoencoder with hidden layers that have more neuors\n",
        "# then the input or serveral hidden layers (i.e. deep autoencoder). In order \n",
        "# to learn sensible encoding in this case when then need to add some form of \n",
        "# regularization otherwise the encoding we simple be the equal to the inputs. \n",
        "# You can find an example for Autoencoders with sparcity constraints here:\n",
        "# https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "\n",
        "# Try if you can improve the performance of our autoencoder above, by \n",
        "# implementing such a sparse autoencoder.\n",
        "\n",
        "\n",
        "\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}