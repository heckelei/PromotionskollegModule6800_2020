{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.7.6-final"
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "labIntro",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heckelei/PromotionskollegModule6800_2020/blob/master/labIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k10_r9GL8YlT",
        "colab_type": "text"
      },
      "source": [
        "### Intro notebook for the course: \"Machine Learning in applied economic analysis\"\n",
        "### Promotionskolleg Module 6800, August 24-28 \n",
        "\n",
        "#### Instructors\n",
        "- Kathy Baylis\n",
        "- Thomas Heckelei\n",
        "- Hugo Storm\n",
        "\n",
        "#### Description\n",
        "This notebook is intended to get you familiar with some of the most common data science / ML libaries typically used in python. \n",
        "In this notebook you will 1) load data, 2) prepare the data for running your models, 3) run a simple logistic regression, 4) run a very simiple neural network, and 5) compare the results of the two models. You will learn in the course that a logistic regression is actually a special case of a very simple neural network! So if you have run a logistic regression you have actually worked with NN... \n",
        "\n",
        "Work Steps\n",
        "\n",
        "1. (If you are reading this in Github and haven't yet opened it in colab,) Open this notebook in google colab (https://colab.research.google.com/) using the link provided above. To run the notebook you need to have a google account. \n",
        "\n",
        "2. Execute all code cells below (Runtime/Run all) and try to understand what is going on.\n",
        "\n",
        "3. Two important python libraries for working with data in python are numpy and pandas \n",
        "    There are plenty of tutorials online to get you a first idea of how they work. Two examples are provided here. For taking the course you do not have to be an expert in using those libraries but having a first basic understanding of the functionality will certainly help you to follow the examples. \n",
        "    \n",
        "- Numpy: https://www.datacamp.com/community/tutorials/python-numpy-tutorial\n",
        "\n",
        "- Pandas: https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html#min   \n",
        "\n",
        "4. (Optional) Play around with the notebook and make some changes (no worries you can not break it...). Here are some ideas what you can try to achieve:\n",
        "\n",
        "- In the data set there are many more variables. Figure out how they are named and add a couple more variables to two models. Run the models and see how this changes the quality of the model prediction (in terms of R²). \n",
        "\n",
        "-  If you want to go a step further... Create some new variables by adding interaction terms or square/cube terms. See if this increases model performance (R²).\n",
        "\n",
        "- Are you up for the challenge (before even starting with the course)? The sklearn libary implements a large number of ML models. We will cover the most important ones in this course. In this notebook you have already seen how to use the logistic regression or a neural network in sklearn. Try to adjust the code to run an additional model, for example a random forest (will be covered on day 2 in the course). There are plenty of tutorials online (for example https://www.datacamp.com/community/tutorials/random-forests-classifier-python). Hint: there is basically only one line of code that you need to change in order to run an random forest with sklearn instead of a logistic regression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9uNLCh58YlU",
        "colab_type": "text"
      },
      "source": [
        "#### Load relevant libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_wnzeZx8YlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import scipy.stats as stat\n",
        "from scipy.stats import norm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01gwbHWy8Ylc",
        "colab_type": "text"
      },
      "source": [
        "#### Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rQhcbrS8Ylh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download data\n",
        "!wget http://www.ilr.uni-bonn.de/agpo/courses/ml/brazil_all_data_v2.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfWHYVAT8Ylk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data with pandas into a dataframe \n",
        "df = pd.read_csv('brazil_all_data_v2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WUBlF368Ylr",
        "colab_type": "text"
      },
      "source": [
        "#### Setup dependent and explantory variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivZFEoz28Yls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define binary variable for deforestration called D_defor_2018 from defor_2018\n",
        "df['D_defor_2018'] = df['defor_2018']>0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWl0aefa8Ylx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add a variable, called constant, with only ones to the dataframe\n",
        "df['constant'] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8gzlvci8Yl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View first 5 rows of the data\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WU_yntf8Yl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the dependent variable\n",
        "Y = df['D_defor_2018']\n",
        "# Define a list of variable names for explanatory variables\n",
        "lstCols = [\n",
        "  'wdpa_2017',\n",
        "  'population_2015',\n",
        "  'chirps_2017',\n",
        "  'defor_2017',\n",
        "  'maize',\n",
        "  'soy',\n",
        "  'sugarcane',\n",
        "  'perc_treecover',\n",
        "  'perm_water',\n",
        "  'travel_min',\n",
        "  'cropland',\n",
        "  # 'pasture',\n",
        "  'mean_elev',\n",
        "  'sd_elev',\n",
        "  'near_road',\n",
        "  'defor_2017_lag_1st_order',\n",
        "  'wdpa_2017_lag_1st_order',\n",
        "  'chirps_2017_lag_1st_order',\n",
        "  'population_2015_lag_1st_order',\n",
        "  'maize_lag_1st_order',\n",
        "  'soy_lag_1st_order',\n",
        "  'sugarcane_lag_1st_order',\n",
        "  'perc_treecover_lag_1st_order',\n",
        "  'perm_water_lag_1st_order',\n",
        "  'travel_min_lag_1st_order',\n",
        "  'cropland_lag_1st_order',\n",
        "  # 'pasture_lag_1st_order',\n",
        "  'mean_elev_lag_1st_order',\n",
        "  'sd_elev_lag_1st_order',\n",
        "  'near_road_lag_1st_order',\n",
        "#  'bean',\n",
        "#  'carrot',\n",
        "#  'cassava',\n",
        "#  'chickpea',\n",
        "#  'citrus',\n",
        "#  'coffee',\n",
        "#  'groundnut',\n",
        "#  'maize',\n",
        "#  'soy',\n",
        "#  'sugarcane',\n",
        "#  'tomato',\n",
        "#  'wheat',\n",
        "#  'defor_2001',\n",
        "#  'defor_2002',\n",
        "#  'defor_2003',\n",
        "#  'defor_2004',\n",
        "#  'defor_2005',\n",
        "#  'defor_2006',\n",
        "#  'defor_2007',\n",
        "#  'defor_2008',\n",
        "#  'defor_2009',\n",
        "#  'defor_2010',\n",
        "#  'defor_2011',\n",
        "#  'defor_2012',\n",
        "#  'defor_2013',\n",
        "#  'defor_2014',\n",
        "#  'defor_2015',\n",
        "#  'defor_2016',\n",
        "#  'defor_2017',\n",
        "#  'near_dist_km',\n",
        "#  'mean_elev_mts',\n",
        "#  'sd_elev_mts',\n",
        " ]\n",
        "\n",
        "# Get the explanatory Variables\n",
        "X =  df.loc[:,lstCols]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgPj9lkr8Yl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into train and test data using sklearn train_test_split object\n",
        "#   (see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "\n",
        "#   Note: This randomly splits the data in 80% train and 20% test data\n",
        "X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVlC9obC8ggx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_raw.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFhM3lFp8YmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale data to 0-1 range using sklearn MinMaxScalar object. This facilitates training the model \n",
        "# (see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) \n",
        "scaler = MinMaxScaler()\n",
        "# Use only the train data to fit the MinMaxScalar \n",
        "scaler.fit(X_train_raw)\n",
        "\n",
        "# Apply the MinMax transformation to the train and test data \n",
        "X_train = scaler.transform(X_train_raw)\n",
        "X_test = scaler.transform(X_test_raw)\n",
        "# Note the depended variable does not need to be scaled as it is a binary variable anyway"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCc0Axm79fvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traindf = pd.DataFrame(X_train)\n",
        "traindf.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F45G02gc8YmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit a logistic regression model using sklearn (see: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "# Create the model object\n",
        "modelLg = LogisticRegression(random_state=0,penalty='none',fit_intercept=True, max_iter=1000)\n",
        "# Fit the model using the training data\n",
        "modelLg.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-yYdgmJ8YmM",
        "colab_type": "text"
      },
      "source": [
        "### Note: \n",
        "sklearn is a popular ML libary that we will primarily use in the course. While sklearn allows to run\n",
        "regressions it does not provide regression table outputs (with p-values, standard errors etc.). \n",
        "While those table are very common in econometrics they are not commonly considered in the ML \n",
        "community. For illustrative puposes we do the calculation for a regression table manually, however,\n",
        "there is also a \"statsmodels\" libary in python that does this automatically (see below). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWVvG9Nb8YmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate pvalues and standard errors for a scikit-learn logisticRegression\n",
        "# Source: https://stackoverflow.com/questions/25122999/scikit-learn-how-to-check-coefficients-significance\n",
        "def logit_pvalue(model, x):\n",
        "    \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
        "    parameters:\n",
        "        model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
        "        x:     matrix on which the model was fit\n",
        "    This function uses asymtptics for maximum likelihood estimates.\n",
        "    \"\"\"\n",
        "    p = model.predict_proba(x)\n",
        "    n = len(p)\n",
        "    m = len(model.coef_[0]) + 1\n",
        "    # m = len(model.coef_[0])\n",
        "    # coefs = model.coef_[0]\n",
        "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
        "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
        "    ans = np.zeros((m, m))\n",
        "    for i in range(n):\n",
        "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
        "    vcov = np.linalg.inv(np.matrix(ans))\n",
        "    se = np.sqrt(np.diag(vcov))\n",
        "    t =  coefs/se  \n",
        "    p = (1 - norm.cdf(abs(t))) * 2\n",
        "    return se, p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ORdUGhf8YmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the previously created function to create a regression output table\n",
        "se, p = logit_pvalue(modelLg, X_train)\n",
        "coefs = np.concatenate([modelLg.intercept_, modelLg.coef_[0]]).T\n",
        "resCoef = pd.DataFrame(coefs,index=['constant']+lstCols)\n",
        "resCoef.columns = ['coef']\n",
        "resCoef['se'] = se\n",
        "resCoef['pval'] = p\n",
        "resCoef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRYFl12K8YmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confirm the results using statsmodels\n",
        "import statsmodels.api as sm\n",
        "# Add constant to X matrix\n",
        "X_train_const = np.matrix(np.insert(np.array(X_train), 0, 1, axis = 1))\n",
        "\n",
        "# Define the logit regression\n",
        "logit = sm.Logit(Y_train,X_train_const)\n",
        "\n",
        "# Set the names of the explanatory variables\n",
        "logit.data.xnames = exog_names=['const']+lstCols\n",
        "\n",
        "# fit the model\n",
        "result = logit.fit()\n",
        "# Print the summary table\n",
        "print(result.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwbd1OKA8Ymg",
        "colab_type": "text"
      },
      "source": [
        "## Train your first very (very) simple neural network using sklearn\n",
        "Now use a neural network for the same problem. In the course you will see that this is actually equivalent to a logistic regression, hence a logistic regression is in fact a specific form of a neural network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_6bKms58Ymg",
        "colab_type": "text"
      },
      "source": [
        "### Perform a hyper parameter search to tune the learning rate for training the NN. \n",
        "This step is optional and takes a while. You can also run the next cell, \n",
        "using a fixed learning rate. The learning rate was determined using this hyper parameter search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwpfQ5I8Ymi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.fixes import loguniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'alpha':loguniform(1e-6, 1e-1)}\n",
        "\n",
        "modelNN = MLPClassifier(solver='lbfgs', activation = 'identity',\n",
        "                     hidden_layer_sizes=(1), random_state=1, verbose=True,max_iter=200)\n",
        "\n",
        "\n",
        "clf = RandomizedSearchCV(modelNN, param_grid, random_state=0,n_iter=10,cv=5)\n",
        "modelNN = clf.fit(X_train_const, Y_train)\n",
        "modelNN.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuVenVfW8Ymk",
        "colab_type": "text"
      },
      "source": [
        "### Train the Neural Network with a fixed set of hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPdSWMNX8Yml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelNN = MLPClassifier(solver='lbfgs', alpha=8.264328927007723e-05,activation = 'identity',\n",
        "                     hidden_layer_sizes=(1), random_state=1, verbose=True,max_iter=200)\n",
        "\n",
        "modelNN.fit(X_train_const, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N3xe5ve8Ymo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add the estimated coefficient of the NN to the regression table we created above-\n",
        "# In the course we will discuss why the estimated coefficient are similar. \n",
        "#    modelNN.coefs_[0] are the coefficients of the first layer\n",
        "#    modelNN.coefs_[1][0][0] is the coefficients of the hidden layer\n",
        "resCoef['coef_NN'] = modelNN.coefs_[0]*modelNN.coefs_[1][0][0]\n",
        "resCoef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGP8hkW28Yms",
        "colab_type": "text"
      },
      "source": [
        "### Compare the model outcomes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT1FnBrn8Ymu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add constant to the test data\n",
        "X_test_const = np.matrix(np.insert(np.array(X_test), 0, 1, axis = 1))\n",
        "# Get predicted values from logit model \n",
        "Y_test_Lg = modelLg.predict(X_test)\n",
        "# Get predicted values from NN model \n",
        "Y_test_NN = modelNN.predict(X_test_const)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um1zNIHj8Ymx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_Lg = np.sum(Y_test==Y_test_Lg)/Y_test.shape[0]\n",
        "score_NN = np.sum(Y_test==Y_test_NN)/Y_test.shape[0]\n",
        "print('Score lg (R²): ',score_Lg)\n",
        "print('Score NN (R²): ',score_NN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKKjn2W58Ym2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the predicted probabalities of the logit and NN models\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "\n",
        "pd.DataFrame(modelLg.predict_proba(X_test))[1].hist(bins=100,ax=ax1)\n",
        "pd.DataFrame(modelNN.predict_proba(X_test_const))[1].hist(bins=100,ax=ax2)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerYBK448Ym5",
        "colab_type": "text"
      },
      "source": [
        "### Well done!!! \n",
        "Now it is your turn. Play around with the notebook to make your very first steps with numpy/pandas and sklearn. In the intro text in the beginning there are some suggestions of what you can try.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTeZV4aa8Ym6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}